# -*- coding: utf-8 -*-
"""
Created on Wed Mar  8 10:03:58 2023

This script calculates the losses for the set of 4800 multivariate forecasts 
generated by 'parallel_mArch_main.py', using the following Stein loss used in
Laurent et. al. (2012):
    
    L_t = tr(H_t^{-1} * \Theta_t) - \ln | H_t^{-1} * \Theta_t | - n
    
Where H_t is the forecasted variance-covariance matrix and \Theta_t is a
quadratic variation proxy. The resulting losses enter the MCS analysis carried
out in the paper.

@author: Sylvain Barde, University of Kent
"""
import numpy as np
import pandas as pd
import pickle
import zlib
import time
import os
from numpy.linalg import LinAlgError

scale = 100
split_obs = 1500

data_path = 'data/oxfordmanrealizedvolatilityindices.csv'
savePath = 'losses'

startList = [200, 1250]     # List of start points

# List of HPC-run forecasts
forecasts = ['forecasts/mArch_sample_0_Run_22-Apr-2024_12-53-45', 
              'forecasts/mArch_sample_1_Run_22-Apr-2024_23-18-21']

rvMeasures = ['rv5',
              'rv5_ss',
              'rv10',
              'rv10_ss',
              'bv',
              'bv_ss',
              'rsv',
              'rsv_ss',
              'rk_twoscale',
              'rk_parzen',
              'rk_th2',
              'medrv']

indList = ['.SPX',
           '.IXIC',
           '.FCHI',
           '.FTSE',
           '.STOXX50E',
           '.AORD',
           '.HSI',
           '.N225',
           '.KS11',
           '.SSEC']

#-----------------------------------------------------------------------------
# Load data and iterate over volatility proxies
data = pd.read_csv(data_path, index_col=[0])
for rvInd in ['rv5','open_to_close']: 
    tStart = time.time()
    print('Processing {:s} volatitily proxy'.format(rvInd))
    
    # Extract data using volatility indicator
    for count,ind in enumerate(indList):
        if count == 0:
            mRt = pd.DataFrame(          # daily returns (for sign)
                    data[data.Symbol == ind]['open_to_close']
                    ).rename(columns={"open_to_close": ind})
                    
            
            mRv = pd.DataFrame(          # volatility predictor
                    data[data.Symbol == ind][rvInd]
                    ).rename(columns={rvInd: ind})
        else:
            mRt = mRt.merge(pd.DataFrame(
                data[data.Symbol == ind]['open_to_close']
                ).rename(columns={"open_to_close": ind}),
                            how='inner', 
                            left_index = True, 
                            right_index = True)
        
            mRv = mRv.merge(pd.DataFrame(
                data[data.Symbol == ind][rvInd]
                ).rename(columns={rvInd: ind}),
                            how='inner', 
                            left_index = True, 
                            right_index = True)

    # Scale, take root and assign sign to RV measure
    if rvInd == 'open_to_close':
        mRv = scale*mRt
    else:
        mRv = scale*np.sign(mRt)*mRv**0.5
    mRv[mRv == 0] = 0.001   # Protect against 0-values
    #--------------------------------------------------------------------------
    # Iterate over samples (high/low volatility)
    for sample, start in zip([0,1], startList):
        print('- Processing sample {:d}'.format(sample))
    
        # Construct test as lead of selected proxy measure
        testData = np.asarray(
                    mRv.iloc[start+split_obs+1:start+split_obs+550+1 + 5])
        S_all = np.abs( testData[:,:,None] @ testData[:,None,:])
        
        # Calculate losses corresponding to each forecast
        lossArray = None
        numBatches = 96
        estimationCheck = np.zeros(50*numBatches,dtype = bool)
        forecastLenCheck = np.zeros(50*numBatches,dtype = np.int64)
        modelIDs = []
        missingIDs_unEst = []
        missingIDs_nanFrcst = []
        missingIDs_negLoss = []
        univarType = {}
        
        # Iterate over the 96 batches
        for baseID in range(numBatches):
        
            fil = open(forecasts[sample] + 
                       '/batch_{:d}-{:d}.pkl'.format(baseID*50,
                                                     (baseID+1)*50),
                       'rb')
            datas = zlib.decompress(fil.read(),0)
            fil.close()
            outDict = pickle.loads(datas,encoding="bytes")
            
            # iterate over dict of forecasts
            for key in outDict.keys(): # keys are numeric here
                    
                univarType[key] = outDict[key]['model']['type']
                estimationCheck[key] = outDict[key]['isEstimated']
                
                # Unestimated models not included
                if outDict[key]['isEstimated']:
            
                    H_all = outDict[key]['forecasts']
                    minForecastLen = 550
                    for i in range(5):
                        minForecastLen = min(
                                        minForecastLen,
                                        H_all['h.{:d}'.format(i+1)].shape[0]
                                        )  
                    
                    forecastLenCheck[key] = minForecastLen
                    
                    # Incomplete forecasts (NaNs) not included
                    if minForecastLen==550:         
                        modelLoss = np.zeros([550,5])
                        
                        # Calculate Stein loss for all 5 horizons
                        for i in range(5):
                            
                            S = S_all[i:i+550]
                            H = H_all['h.{:d}'.format(i+1)]
        
                            try:    # Control for singular matrices
                                modelLoss[:,i] = (
                                    np.trace(np.matmul(np.linalg.inv(H),S), 
                                                        axis1=1, axis2=2) 
                                    - np.linalg.slogdet(S)[1] 
                                    + np.linalg.slogdet(H)[1] - 10
                                    )
                                
                            except LinAlgError as err:
                                if "Singular matrix" in str(err):
                                # If H is singular, add some jitter
                                
                                    H += np.eye(10)*0.001
                                    modelLoss[:,i] = (
                                        np.trace(np.matmul(np.linalg.inv(H),S), 
                                                            axis1=1, axis2=2) 
                                        - np.linalg.slogdet(S)[1] 
                                        + np.linalg.slogdet(H)[1] - 10
                                        )
                                else:
                                    raise                                        
                                
                            
                        # Pathological (negative) losses not included
                        if not np.any(modelLoss < 0): 
                            modelIDs.append(key)
                            if lossArray is None:
                                lossArray = modelLoss[:,None,:]
                            else:
                                lossArray = np.concatenate(
                                    (lossArray,modelLoss[:,None,:]),
                                                            axis=1)
                        else:
                            missingIDs_negLoss.append(key)    
                    else:
                        missingIDs_nanFrcst.append(key)
                else:
                    missingIDs_unEst.append(key)
                            
        # Get diagnostics on failed forecasts
        missingMods = set(np.arange(4800)) - set(modelIDs)
        checksum = missingMods == set(missingIDs_negLoss + 
                                      missingIDs_nanFrcst +
                                      missingIDs_unEst)
        
        # Generate aggregate diagnostics
        batchmissing = np.floor(np.asarray(list(missingMods))/50)*50
        values, counts = np.unique(batchmissing, return_counts=True)
        missingDiagnostics = np.concatenate((values.astype(np.int32)[:,None],
                                             counts.astype(np.int32)[:,None]),
                                            axis = 1)
        # Generate detailed diagnostics
        missingDetail = np.zeros([len(values),3])
        for i,missingTypeList in enumerate([missingIDs_unEst,
                                            missingIDs_nanFrcst,
                                            missingIDs_negLoss]):
            for ind in missingTypeList:
                batchInd = int(np.floor(ind/50)*50)
                missingDetail[np.where(values == batchInd),
                              i] += 1
        missingDiagnostics = np.concatenate((missingDiagnostics,
                                             missingDetail.astype(np.int32)), 
                                            axis = 1)
        
        # Reconstruct batch settings and map to failed forecasts
        errorLst = ['Normal','Student']
        multivarLst = ['naive','ccc','dcc','dcca']
        lst = [errorLst,multivarLst,rvMeasures]
        settingsArray = np.array(np.meshgrid(*lst)).T.reshape(-1,len(lst))
        batchIDs = np.arange(0,4800,50)
        
        fullModelSpecs = np.concatenate((batchIDs[:,None],
                                         settingsArray), axis = 1)
        missingModelSpecs = np.empty(shape=(0,settingsArray.shape[1]))
        for value in values:
            missingModelSpecs = np.concatenate(
                (missingModelSpecs, 
                 settingsArray[np.where(batchIDs == value)]
                 ), axis = 0)
        
        missingDiagnostics = np.concatenate((missingDiagnostics,
                                             missingModelSpecs), axis = 1)
        
        missingDiagnosticsDf = pd.DataFrame(data = missingDiagnostics,
                                            columns = ['batch ID',
                                                      'Total',
                                                       'Not estimated',
                                                       'Incomplete',
                                                       'Neg. loss',
                                                       'error',
                                                       'mArch',
                                                       'RV ind'])
        # Package losses and save
        lossOut = {'losses':lossArray,
                   'modList':modelIDs,
                   'missingMods':missingMods,
                   'missingCheck':checksum,
                   'missingDiagnostics':missingDiagnosticsDf,
                   'modelSpecs':fullModelSpecs,
                   'univarType':univarType}
        
        # Save zipped losses
        if not os.path.exists(savePath):
            os.makedirs(savePath,mode=0o777)
        fileName = 'sample_{:d}_proxy_{:s}'.format(sample,rvInd)
        fil = open(savePath + '//' + fileName + '.pkl','wb') 
        fil.write(zlib.compress(pickle.dumps(lossOut, protocol=2)))
        fil.close()
    
        print('  Time taken: {:10.4f} secs'.format(time.time() - tStart))
#------------------------------------------------------------------------------